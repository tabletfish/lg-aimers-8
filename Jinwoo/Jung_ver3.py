import os
import torch
import shutil
import json
import torch.nn as nn
from datasets import load_dataset
# ìµœì‹  trlì—ì„œëŠ” SFTConfigë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
from trl import SFTTrainer, SFTConfig 
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType
from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier
from google.colab import drive, files

# 0. ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸
if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

MODEL_ID = "/content/drive/MyDrive/Colab/base_model"
OUT_DIR  = "./model"
DATASET_ID = "LGAI-EXAONE/MANTA-1M"

# ==========================================
# 1. ëª¨ë¸ ë¡œë“œ & Pruning
# ==========================================
print("[INFO] 1. ëª¨ë¸ ë¡œë“œ ë° Pruning...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token 

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    attn_implementation="flash_attention_2"
)

# Pruning (ì•ˆì „í•œ 5ê°œ ì‚­ì œ)
layers_to_drop = {9, 11, 13, 17, 19}
if hasattr(model, "model") and hasattr(model.model, "layers"):
    old_layers = model.model.layers
else:
    old_layers = model.model.layers # Fallback

new_layers = nn.ModuleList()
for i, layer in enumerate(old_layers):
    if i not in layers_to_drop:
        new_layers.append(layer)
    else:
        del layer
        
model.model.layers = new_layers
model.config.num_hidden_layers = len(new_layers)
print(f"   -> Pruning ì™„ë£Œ: {len(old_layers)} -> {len(new_layers)}")

# ==========================================
# 2. LoRA íŒŒì¸ íŠœë‹ (ìµœì‹  trl ë¬¸ë²• ì ìš©)
# ==========================================
print("[INFO] 2. LoRA íŒŒì¸ íŠœë‹...")

peft_config = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# ë°ì´í„° í¬ë§·íŒ… (ë¦¬ìŠ¤íŠ¸ ì—ëŸ¬ ë°©ì§€ìš© ìˆ˜ë™ ì „ì²˜ë¦¬)
def format_data(batch):
    formatted = []
    for convo in batch['conversations']:
        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)
        formatted.append(text)
    return {"text": formatted}

train_dataset = load_dataset(DATASET_ID, split="train")
train_dataset = train_dataset.shuffle(seed=42).select(range(2000))
train_dataset = train_dataset.map(format_data, batched=True, remove_columns=train_dataset.column_names)

# [í•µì‹¬] SFTConfig ì‚¬ìš© (ìµœì‹  ë²„ì „ì€ ëª¨ë“  ì„¤ì •ì„ ì—¬ê¸° ë„£ì–´ì•¼ í•¨)
sft_config = SFTConfig(
    output_dir="./lora_output",
    dataset_text_field="text",   # ë°ì´í„° ì»¬ëŸ¼ëª…
    max_seq_length=2048,         # ì‹œí€€ìŠ¤ ê¸¸ì´
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    learning_rate=2e-4,
    max_steps=100,
    fp16=False,
    bf16=True,
    optim="adamw_torch",
    save_strategy="no",
    report_to="none",
    packing=False
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    args=sft_config,       # Config ê°ì²´ ì „ë‹¬
    peft_config=peft_config,
    # ì—¬ê¸°ì— dataset_text_field ê°™ì€ ê±° ë„£ìœ¼ë©´ ì—ëŸ¬ë‚¨ (Configì— ë„£ì—ˆìœ¼ë‹ˆ ì œê±°)
)

trainer.train()
print("[INFO] LoRA í•™ìŠµ ì™„ë£Œ ë° ë³‘í•©...")
model = trainer.model.merge_and_unload()

# ==========================================
# 3. GPTQ ì–‘ìí™”
# ==========================================
print("[INFO] 3. GPTQ ì–‘ìí™”...")
calib_dataset = load_dataset(DATASET_ID, split="train").shuffle(seed=42).select(range(2000, 2512))

def preprocess_calib(example):
    return {"text": tokenizer.apply_chat_template(example["conversations"], add_generation_prompt=True, tokenize=False)}
ds_calib = calib_dataset.map(preprocess_calib)

recipe = [
    GPTQModifier(scheme="W4A16", targets=["Linear"], ignore=["embed_tokens", "lm_head"], dampening_frac=0.01)
]

oneshot(
    model=model, dataset=ds_calib, recipe=recipe,
    max_seq_length=2048, num_calibration_samples=512
)

# ==========================================
# 4. ì €ì¥ ë° ì œì¶œ
# ==========================================
print("[INFO] 4. ì €ì¥ ë° ì••ì¶•...")
if os.path.exists(OUT_DIR): shutil.rmtree(OUT_DIR)
os.makedirs(OUT_DIR, exist_ok=True)

model.save_pretrained(OUT_DIR, save_compressed=True)
tokenizer.save_pretrained(OUT_DIR)

# Config ìˆ˜ì •
config_path = os.path.join(OUT_DIR, "config.json")
with open(config_path, "r") as f: config = json.load(f)
config["num_hidden_layers"] = len(new_layers)

# ğŸ”¥ layer_typesë„ ê°™ì´ ì˜ë¼ì£¼ê¸°
if "layer_types" in config:
    config["layer_types"] = config["layer_types"][:len(new_layers)]

if "architectures" not in config: config["architectures"] = ["ExaoneForCausalLM"]
with open(config_path, "w") as f: json.dump(config, f, indent=2)

shutil.make_archive("final_submission_v5", "zip", root_dir=".", base_dir="model")
try: files.download("final_submission_v5.zip")
except: print("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ")